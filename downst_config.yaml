trainer:
  fit:
    ckpt_path: /cra-614/checkpoints/cs/Llama-3.2-1B/pytorch_model_to_cs-2.3.mdl
    train_dataloader:
      batch_size: 512
      data_processor: GptHDF5MapDataProcessor
      mixture:
      - data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_8/train
        weight: 0.13131313131313133
        data_subset: 0.0000000000000000-1.0
      - data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_9/train
        weight: 0.18181818181818182
        data_subset: 0.0000000000000000-1.0
      - data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_16/train
        weight: 0.030303030303030304
        data_subset: 0.0000000000000000-1.0
      - data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_18/train
        weight: 0.22222222222222224
        data_subset: 0.0000000000000000-1.0
      - data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_19/train
        weight: 0.43434343434343436
        data_subset: 0.0000000000000000-1.0
      num_workers: 8
      persistent_workers: true
      prefetch_factor: 10
      shuffle: false
    val_dataloader:
    - data_processor: GptHDF5MapDataProcessor
      data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_8/valid
      batch_size: 512
    - data_processor: GptHDF5MapDataProcessor
      data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_9/valid
      batch_size: 512
    - data_processor: GptHDF5MapDataProcessor
      data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_16/valid
      batch_size: 512
    - data_processor: GptHDF5MapDataProcessor
      data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_18/valid
      batch_size: 512
    - data_processor: GptHDF5MapDataProcessor
      data_dir: /cra-614/ml/ganeshv/llama3/pt/CLIMB/climb_mix/split_h5/cluster_19/valid
      batch_size: 512
  init:
    model_dir: /cra-614/workdirs/11062025_data_mix_expt/artifacts/downstream_0.1_smoothing_0.5_downstream_importance
    backend:
      backend_type: CSX
    callbacks:
    - ComputeNorm: {}
    - GlobalFlags:
        csx.debug.save_gpv: false
    - EleutherEvalHarness:
        eeh_args:
          output_path: /cra-614/workdirs/11062025_data_mix_expt/artifacts/downstream_0.1_smoothing_0.5_downstream_importance/eval_dir/arc_challenge/checkpoint_1198
          tasks: arc_challenge
          num_fewshot: 0
          log_samples: true
        keep_data_dir: false
        flags:
          csx.performance.micro_batch_size: null
        # Dataloader settings
        batch_size: 256
        shuffle: false
        max_sequence_length: 8192
        num_workers: 1
        data_dir: /cra-614/workdirs/250609_data_mixing_efficiency_experiments/configs/climb_baseline_mix/evals/downstream/data_dir/
        eos_id: 128001
        start_token: 128000
        stop_sequences: []
        max_tokens: 256
        pretrained_model_name_or_path: meta-llama/Llama-3.2-1B
    - EleutherEvalHarness:
        eeh_args:
          output_path: /cra-614/workdirs/11062025_data_mix_expt/artifacts/downstream_0.1_smoothing_0.5_downstream_importance/eval_dir/arc_easy/checkpoint_1198
          tasks: arc_easy
          num_fewshot: 0
          log_samples: true
        keep_data_dir: false
        flags:
          csx.performance.micro_batch_size: null
        # Dataloader settings
        batch_size: 256
        shuffle: false
        max_sequence_length: 8192
        num_workers: 1
        data_dir: /cra-614/workdirs/250609_data_mixing_efficiency_experiments/configs/climb_baseline_mix/evals/downstream/data_dir/
        eos_id: 128001
        start_token: 128000
        stop_sequences: []
        max_tokens: 256
        pretrained_model_name_or_path: meta-llama/Llama-3.2-1B
    - EleutherEvalHarness:
        eeh_args:
          output_path: /cra-614/workdirs/11062025_data_mix_expt/artifacts/downstream_0.1_smoothing_0.5_downstream_importance/eval_dir/mmlu/checkpoint_1198
          tasks: mmlu
          num_fewshot: 0
          log_samples: true
        keep_data_dir: false
        flags:
          csx.performance.micro_batch_size: null
        # Dataloader settings
        batch_size: 256
        shuffle: false
        max_sequence_length: 8192
        num_workers: 1
        data_dir: /cra-614/workdirs/250609_data_mixing_efficiency_experiments/configs/climb_baseline_mix/evals/downstream/data_dir/
        eos_id: 128001
        start_token: 128000
        stop_sequences: []
        max_tokens: 256
        pretrained_model_name_or_path: meta-llama/Llama-3.2-1B
    checkpoint:
      disable_strict_checkpoint_loading: true
      save_initial_checkpoint: false
      steps: 599
    logging:
      log_steps: 1
    loop:
      eval_frequency: 1198
      max_steps: 9578
    model:
      attention_dropout_rate: 0.0
      attention_module: multiquery_attention
      attention_type: scaled_dot_product
      dropout_rate: 0.0
      embedding_dropout_rate: 0.0
      embedding_layer_norm: false
      extra_attention_params:
        num_kv_groups: 8
      filter_size: 8192
      hidden_size: 2048
      initializer_range: 0.02
      layer_norm_epsilon: 1.0e-05
      loss_scaling: num_tokens
      loss_weight: 1.0
      max_position_embeddings: 8192
      name: llama
      nonlinearity: swiglu
      norm_type: rmsnorm
      num_heads: 32
      num_hidden_layers: 16
      pos_scaling_extra_args:
        high_freq_factor: 4.0
        low_freq_factor: 1.0
        original_max_position_embeddings: 8192
      pos_scaling_factor: 32.0
      pos_scaling_type: llama3
      position_embedding_type: rotary
      rope_theta: 500000.0
      rotary_dim: 64
      share_embedding_weights: true
      use_bias_in_output: false
      use_ffn_bias: false
      use_ffn_bias_in_attention: false
      use_projection_bias_in_attention: false
      vocab_size: 128256
    optimizer:
      AdamW:
        betas:
        - 0.9
        - 0.95
        correct_bias: true
        weight_decay: 0.1
    precision:
      enabled: true
      fp16_type: cbfloat16
      log_loss_scale: true
      loss_scaling_factor: dynamic
      max_gradient_norm: 1.0
      precision_opt_level: 1
    schedulers:
    - SequentialLR:
        schedulers:
        - CosineDecayLR:
            end_learning_rate: 0.0003
            initial_learning_rate: 0
            total_iters: 95
        - CosineDecayLR:
            end_learning_rate: 3.0e-05
            initial_learning_rate: 0.0003
            total_iters: 9483
    seed: 1
